# Sentiment-Analysis

### Project Background

Helio is working with a government health agency to create a suite of smart phone medical apps 
for use by aid workers in developing countries. This suite of apps will enable the aid workers 
to manage local health conditions by facilitating communication with medical professionals located 
elsewhere (one of the apps, for example, enables specialists in communicable diseases to diagnose 
conditions by examining images and other patient data uploaded by local aid workers). The government 
agency requires that the app suite be bundled with one model of smart phone. Helio is in the process 
of evaluating potential handset models to determine which one to bundle their software with. After 
completing an initial investigation, Helio has created a short list of five devices that are all capable 
of executing the app suite’s functions. To help Helio narrow their list down to one device, they have 
asked us to examine the prevalence of positive and negative attitudes toward these devices on the web. 
The goal of this project is to provide our client with a report that contains an analysis of sentiment 
toward the target devices, as well as a description of the methods and processes we used to arrive at our 
conclusions.

### Approach to the Project

Although there are a number of ways to capture sentiment from text documents, our general approach to 
this project is to count words associated with sentiment toward these devices within relevant documents on 
the web. We then leverage this data and machine learning methods to look for patterns in the documents that 
enable us to label each of these documents with a value that represents the level of positive or negative 
sentiment toward each of these devices. We then analyze and compare the frequency and distribution of the 
sentiment for each of these devices.

In order to really gauge the sentiment toward these devices, we must do this on a very large scale. To that 
end, we use the cloud computing platform provided by Amazon Web Services (AWS) to conduct the analysis. The 
data sets we analyze will come from Common Crawl. Common Crawl is an open repository of web crawl data 
(over 5 billion pages so far) that is stored on Amazon’s Public Data Sets.

### Project Task

Our job during the course of this project is to collect and develop a data matrix in the range of 20 thousand 
instances (called the large data matrix) of relevant web documents from the Common Crawl. Using Amy’s labeled 
small matrices, you will create models in R that understand the sentiment patterns within the data. Then you 
will apply your models to the large matrices you collected to understand their sentiment scores. Lastly you 
will analyze this large labeled data matrix and report descriptive statistics to the client on the level of 
sentiment toward the handsets.

### Using Amazon Web Services

AWS is a reliable, scalable, and inexpensive platform for the use of cloud applications and services and it 
will give you easy access to web data through one management console. The specific AWS services that you will 
be employing include Elastic Compute Cloud (EC2), Elastic MapReduce (EMR), and Simple Storage Service (S3). 
I’ve outlined these services briefly below:

The Amazon Elastic Compute Cloud (EC2) is a web-based service that enables you to run application programs in 
the Amazon computing environment. EC2 can serve as a practically unlimited set of virtual machines. 
Amazon Elastic MapReduce (EMR) is a web service that enables you to easily and cost-effectively process vast 
amounts of data. It utilizes a hosted Hadoop framework running on the web-scale infrastructure of Amazon Elastic 
Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). This is where you will process the 
Common Crawl data to develop the large data matrix.
Amazon Simple Storage Services (S3) is storage designed to make web-scale computing easier for developers and 
provides a simple web services interface that can be used to store and retrieve any amount of data, at any time, 
from anywhere on the web. This is a general platform that stores all the data (input and output) for the EC2 and 
EMR services.

### Preparing Data

Run a mapper and reducer script on WET files of Common Crawl data in EMR using the EMR console.  This will be repeated
on 400-500 Common Crawn WET files using the EMR command line interface.  We will then use a concatenating script to
compile the Common Crawl data to create a "large matrix" dataset.

#### his task requires you to prepare two deliverables:
* Preliminary Output Generated from the EMR Console— 
a csv file generated after running the Python Mapper and Reducer programs on a single Common Crawl WET file

* Large Data Matrix — 
a csv file generated by running the Python Mapper and Reducer programs on 400 to 500 Common Crawl 
WET files (approximately a billion pages) and meets the following requirements:
  * Is initiated by the command line interface using JSON job files that either you have created or that have been provided 
to you. It is OK to expand upon JSON files provided to crawl more segments of the Common Crawl.
  * Combines the results of all the streaming Hadoop jobs into a single csv file.
  * Contains a minimum of 20,000 instances, but should contain as many as you are reasonably able to gather. This number 
can be increased by expanding the number of common crawl WET file addresses your JSON file specifies for processing.

### Next Steps
* Set up parallel processing
* Explore the Small Matrices to understand the attributes
* Preprocessing & Feature Selection
* Model Development and Evaluation
* Feature Engineering
* Apply Model to Large Matrix and get Predictions
* Analyze results

